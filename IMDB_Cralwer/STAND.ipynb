{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((u'trailer', u'NN'), u'nsubj', (u'This', u'DT'))\n",
      "((u'trailer', u'NN'), u'cop', (u'is', u'VBZ'))\n",
      "((u'trailer', u'NN'), u'det', (u'a', u'DT'))\n",
      "((u'trailer', u'NN'), u'amod', (u'long', u'JJ'))\n",
      "((u'long', u'JJ'), u'advmod', (u'very', u'RB'))\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "path_to_jar = 'D:\\stanford-parser-full-2015-04-20\\stanford-parser.jar'\n",
    "path_to_models_jar = 'D:\\stanford-parser-full-2015-04-20\\stanford-parser-3.5.2-models.jar'\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "result = dependency_parser.raw_parse(\"This is a very long trailer.\")\n",
    "\n",
    "dep = result.next()\n",
    "for ele in list(dep.triples()):\n",
    "    print ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named StanfordDependencies",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-d636552855b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mStanfordDependencies\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStanfordDependencies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'subprocess'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named StanfordDependencies"
     ]
    }
   ],
   "source": [
    "import StanfordDependencies\n",
    "sd = StanfordDependencies.get_instance(backend='subprocess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: __main__.py [OPTIONS]\n",
      "\n",
      "__main__.py: error: no such option: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "#\n",
    "# corenlp  - Python interface to Stanford Core NLP tools\n",
    "# Copyright (c) 2012 Dustin Smith\n",
    "#   https://github.com/dasmith/stanford-corenlp-python\n",
    "#\n",
    "# This program is free software; you can redistribute it and/or\n",
    "# modify it under the terms of the GNU General Public License\n",
    "# as published by the Free Software Foundation; either version 2\n",
    "# of the License, or (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program; if not, write to the Free Software\n",
    "# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n",
    "\n",
    "\n",
    "import json\n",
    "import optparse\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import traceback\n",
    "import pexpect\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "\n",
    "from subprocess import call\n",
    "\n",
    "VERBOSE = False\n",
    "STATE_START, STATE_TEXT, STATE_WORDS, STATE_TREE, STATE_DEPENDENCY, STATE_COREFERENCE = 0, 1, 2, 3, 4, 5\n",
    "WORD_PATTERN = re.compile('\\[([^\\]]+)\\]')\n",
    "#CR_PATTERN = re.compile(r\"\\((\\d*),(\\d)*,\\[(\\d*),(\\d*)\\]\\) -> \\((\\d*),(\\d)*,\\[(\\d*),(\\d*)\\]\\), that is: \\\"(.*)\\\" -> \\\"(.*)\\\"\")\n",
    "\n",
    "# Fixed by Diego Reforgiato. Otherwise src_pos gets one digit only\n",
    "CR_PATTERN = re.compile(r\"\\((\\d*),(\\d*),\\[(\\d*),(\\d*)\\]\\) -> \\((\\d*),(\\d*),\\[(\\d*),(\\d*)\\]\\), that is: \\\"(.*)\\\" -> \\\"(.*)\\\"\")\n",
    "\n",
    "if os.environ.has_key(\"CORENLP\"):\n",
    "    DIRECTORY = os.environ[\"CORENLP\"]\n",
    "else:\n",
    "    DIRECTORY = \"stanford-corenlp-full-2014-08-27\"\n",
    "\n",
    "class bc:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "\n",
    "\n",
    "class ProcessError(Exception):\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n",
    "\n",
    "\n",
    "class ParserError(Exception):\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n",
    "\n",
    "\n",
    "class TimeoutError(Exception):\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n",
    "\n",
    "\n",
    "class OutOfMemoryError(Exception):\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n",
    "\n",
    "\n",
    "def init_corenlp_command(corenlp_path, memory, properties):\n",
    "    \"\"\"\n",
    "    Checks the location of the jar files.\n",
    "    Spawns the server as a process.\n",
    "    \"\"\"\n",
    "\n",
    "    jars = [\"stanford-corenlp-?.?.?.jar\",\n",
    "            \"stanford-corenlp-?.?.?-models.jar\",\n",
    "            \"xom.jar\",\n",
    "            \"joda-time.jar\",\n",
    "            \"jollyday.jar\",\n",
    "            \"ejml-?.*.jar\"] # No idea what this is but it might be sentiment\n",
    "\n",
    "    java_path = \"java\"\n",
    "    classname = \"edu.stanford.nlp.pipeline.StanfordCoreNLP\"\n",
    "\n",
    "    # include the properties file, so you can change defaults\n",
    "    # but any changes in output format will break parse_parser_results()\n",
    "    current_dir_pr = os.path.dirname(os.path.abspath(__file__)) + \"/\" + properties\n",
    "    if os.path.exists(properties):\n",
    "        props = \"-props %s\" % (properties)\n",
    "    elif os.path.exists(current_dir_pr):\n",
    "        props = \"-props %s\" % (current_dir_pr)\n",
    "    else:\n",
    "        raise Exception(\"Error! Cannot locate: %s\" % properties)\n",
    "\n",
    "    # add and check classpaths\n",
    "    jars = [corenlp_path + \"/\" + jar for jar in jars]\n",
    "    missing = [jar for jar in jars if not glob.glob(jar)]\n",
    "    if missing:\n",
    "        raise Exception(\"Error! Cannot locate: %s\" % ', '.join(missing))\n",
    "    jars = [glob.glob(jar)[0] for jar in jars]\n",
    "\n",
    "    # add memory limit on JVM\n",
    "    if memory:\n",
    "        limit = \"-Xmx%s\" % memory\n",
    "    else:\n",
    "        limit = \"\"\n",
    "\n",
    "    return \"%s %s -cp %s %s %s\" % (java_path, limit, ':'.join(jars), classname, props)\n",
    "\n",
    "\n",
    "def remove_id(word):\n",
    "    \"\"\"Removes the numeric suffix from the parsed recognized words: e.g. 'word-2' > 'word' \"\"\"\n",
    "    return word.count(\"-\") == 0 and word or word[0:word.rindex(\"-\")]\n",
    "\n",
    "\n",
    "def parse_bracketed(s):\n",
    "    '''Parse word features [abc=... def = ...]\n",
    "    Also manages to parse out features that have XML within them\n",
    "    '''\n",
    "    word = None\n",
    "    attrs = {}\n",
    "    temp = {}\n",
    "    # Substitute XML tags, to replace them later\n",
    "    for i, tag in enumerate(re.findall(r\"(<[^<>]+>.*<\\/[^<>]+>)\", s)):\n",
    "        temp[\"^^^%d^^^\" % i] = tag\n",
    "        s = s.replace(tag, \"^^^%d^^^\" % i)\n",
    "    # Load key-value pairs, substituting as necessary\n",
    "    for attr, val in re.findall(r\"([^=\\s]*)=([^=\\s]*)\", s):\n",
    "        if val in temp:\n",
    "            val = temp[val]\n",
    "        if attr == 'Text':\n",
    "            word = val\n",
    "        else:\n",
    "            attrs[attr] = val\n",
    "    return (word, attrs)\n",
    "\n",
    "\n",
    "def parse_parser_results(text):\n",
    "    \"\"\" This is the nasty bit of code to interact with the command-line\n",
    "    interface of the CoreNLP tools.  Takes a string of the parser results\n",
    "    and then returns a Python list of dictionaries, one for each parsed\n",
    "    sentence.\n",
    "    \"\"\"\n",
    "    results = {\"sentences\": []}\n",
    "    state = STATE_START\n",
    "\n",
    "    if sys.version_info[0] < 3 and isinstance(text, str) or \\\n",
    "            sys.version_info[0] >= 3 and isinstance(text, bytes):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    for line in text.split('\\n'):\n",
    "        line = line.strip()\n",
    "\n",
    "        if line.startswith(\"Sentence #\"):\n",
    "            sentence = {'words': [], 'parsetree': [], 'dependencies': [], 'indexeddependencies': []}\n",
    "            results[\"sentences\"].append(sentence)\n",
    "            state = STATE_TEXT\n",
    "\n",
    "        elif state == STATE_TEXT:\n",
    "            sentence['text'] = line\n",
    "            state = STATE_WORDS\n",
    "\n",
    "        elif state == STATE_WORDS:\n",
    "            if not line.startswith(\"[Text=\"):\n",
    "                raise ParserError('Parse error. Could not find \"[Text=\" in: %s' % line)\n",
    "            for s in WORD_PATTERN.findall(line):\n",
    "                sentence['words'].append(parse_bracketed(s))\n",
    "            state = STATE_TREE\n",
    "\n",
    "        elif state == STATE_TREE:\n",
    "            if len(line) == 0:\n",
    "                state = STATE_DEPENDENCY\n",
    "                sentence['parsetree'] = \" \".join(sentence['parsetree'])\n",
    "            else:\n",
    "                sentence['parsetree'].append(line)\n",
    "\n",
    "        elif state == STATE_DEPENDENCY:\n",
    "            if len(line) == 0:\n",
    "                state = STATE_COREFERENCE\n",
    "            else:\n",
    "                split_entry = re.split(\"\\(|, \", line[:-1])\n",
    "                if len(split_entry) == 3:\n",
    "                    rel, left, right = map(lambda x: remove_id(x), split_entry)\n",
    "                    sentence['dependencies'].append(tuple([rel, left, right]))\n",
    "                    sentence['indexeddependencies'].append(tuple(split_entry))\n",
    "\n",
    "        elif state == STATE_COREFERENCE:\n",
    "            if \"Coreference set\" in line:\n",
    "                if 'coref' not in results:\n",
    "                    results['coref'] = []\n",
    "                coref_set = []\n",
    "                results['coref'].append(coref_set)\n",
    "            else:\n",
    "                for src_i, src_pos, src_l, src_r, sink_i, sink_pos, sink_l, sink_r, src_word, sink_word in CR_PATTERN.findall(line):\n",
    "                    src_i, src_pos, src_l, src_r = int(src_i) - 1, int(src_pos) - 1, int(src_l) - 1, int(src_r) - 1\n",
    "                    sink_i, sink_pos, sink_l, sink_r = int(sink_i) - 1, int(sink_pos) - 1, int(sink_l) - 1, int(sink_r) - 1\n",
    "                    coref_set.append(((src_word, src_i, src_pos, src_l, src_r), (sink_word, sink_i, sink_pos, sink_l, sink_r)))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def parse_parser_xml_results(xml, file_name=\"\", raw_output=False):\n",
    "    import xmltodict\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    def enforceList(list_or_ordered_dict): #TIM\n",
    "        if type(list_or_ordered_dict) == type(OrderedDict()):\n",
    "            return [list_or_ordered_dict]\n",
    "        else:\n",
    "            return list_or_ordered_dict\n",
    "\n",
    "    def extract_words_from_xml(sent_node):\n",
    "        if type(sent_node['tokens']['token']) == type(OrderedDict()):\n",
    "            # This is also specific case of xmltodict\n",
    "            exted = [sent_node['tokens']['token']]\n",
    "        else:\n",
    "            exted = sent_node['tokens']['token']\n",
    "        exted_string = map(lambda x: x['word'], exted)\n",
    "        return exted_string\n",
    "\n",
    "    # Turning the raw xml into a raw python dictionary:\n",
    "    raw_dict = xmltodict.parse(xml)\n",
    "    if raw_output:\n",
    "        return raw_dict\n",
    "\n",
    "    document = raw_dict[u'root'][u'document']\n",
    "\n",
    "    # Making a raw sentence list of dictionaries:\n",
    "    raw_sent_list = document[u'sentences'][u'sentence']\n",
    "\n",
    "    if document.get(u'coreference') and document[u'coreference'].get(u'coreference'):\n",
    "        # Convert coreferences to the format like python\n",
    "        coref_flag = True\n",
    "\n",
    "        # Making a raw coref dictionary:\n",
    "        raw_coref_list = document[u'coreference'][u'coreference']\n",
    "\n",
    "        # It is specific case that there is only one item for xmltodict\n",
    "        if len(raw_coref_list) == 1:\n",
    "            raw_coref_list = [raw_coref_list]\n",
    "        if len(raw_sent_list) == 1:\n",
    "            raw_sent_list = [raw_sent_list]\n",
    "\n",
    "        # This is also specific case of xmltodict\n",
    "        raw_sent_list = enforceList(raw_sent_list)\n",
    "\n",
    "        # To dicrease is for given index different from list index\n",
    "        coref_index = [[[int(raw_coref_list[j]['mention'][i]['sentence']) - 1,\n",
    "                         int(raw_coref_list[j]['mention'][i]['head']) - 1,\n",
    "                         int(raw_coref_list[j]['mention'][i]['start']) - 1,\n",
    "                         int(raw_coref_list[j]['mention'][i]['end']) - 1]\n",
    "                        for i in xrange(len(raw_coref_list[j][u'mention']))]\n",
    "                       for j in xrange(len(raw_coref_list))]\n",
    "\n",
    "        coref_list = []\n",
    "        for j in xrange(len(coref_index)):\n",
    "            coref_list.append(coref_index[j])\n",
    "            for k, coref in enumerate(coref_index[j]):\n",
    "                if type(raw_sent_list[coref[0]]['tokens']['token']) == type(OrderedDict()):\n",
    "                    # This is also specific case of xmltodict\n",
    "                    exted = [raw_sent_list[coref[0]]['tokens']['token']]\n",
    "                else:\n",
    "                    exted = raw_sent_list[coref[0]]['tokens']['token'][coref[2]:coref[3]]\n",
    "                exted_words = map(lambda x: x['word'], exted)\n",
    "                coref_list[j][k].insert(0, ' '.join(exted_words))\n",
    "\n",
    "        coref_list = [[[coref_list[j][i], coref_list[j][0]]\n",
    "                       for i in xrange(len(coref_list[j])) if i != 0]\n",
    "                      for j in xrange(len(coref_list))]\n",
    "    else:\n",
    "        coref_flag = False\n",
    "\n",
    "    # This is also specific case of xmltodict\n",
    "    raw_sent_list = enforceList(raw_sent_list)\n",
    "\n",
    "    sentences = []\n",
    "    for id in xrange(len(raw_sent_list)):\n",
    "        sent = {}\n",
    "        sent['text'] = extract_words_from_xml(raw_sent_list[id])\n",
    "        sent['parsetree'] = unicode(raw_sent_list[id]['parse'])\n",
    "        # sent['sentimentValue'] = int(raw_sent_list[id].get(['@sentimentValue'])) # TIM\n",
    "        # sent['sentiment'] = raw_sent_list[id]['@sentiment'] # TIM\n",
    "        sentiment_value = raw_sent_list[id].get('@sentimentValue')\n",
    "        sentiment = raw_sent_list[id].get('@sentiment')\n",
    "        if sentiment_value: sent['sentimentValue'] = int(sentiment_value)\n",
    "        if sentiment_value: sent['sentiment'] = sentiment\n",
    "\n",
    "        if type(raw_sent_list[id]['tokens']['token']) == type(OrderedDict()):\n",
    "            # This is also specific case of xmltodict\n",
    "            token = raw_sent_list[id]['tokens']['token']\n",
    "            sent['words'] = [\n",
    "                [unicode(token['word']), OrderedDict([\n",
    "                    ('NamedEntityTag', str(token['NER'])),\n",
    "                    ('CharacterOffsetEnd', str(token['CharacterOffsetEnd'])),\n",
    "                    ('CharacterOffsetBegin', str(token['CharacterOffsetBegin'])),\n",
    "                    ('PartOfSpeech', str(token['POS'])),\n",
    "                    ('Lemma', unicode(token['lemma']))])]\n",
    "            ]\n",
    "        else:\n",
    "            sent['words'] = [[unicode(token['word']), OrderedDict([\n",
    "                ('NamedEntityTag', str(token['NER'])),\n",
    "                ('CharacterOffsetEnd', str(token['CharacterOffsetEnd'])),\n",
    "                ('CharacterOffsetBegin', str(token['CharacterOffsetBegin'])),\n",
    "                ('PartOfSpeech', str(token['POS'])),\n",
    "                ('Lemma', unicode(token['lemma']))])]\n",
    "                             for token in raw_sent_list[id]['tokens']['token']]\n",
    "\n",
    "        sent['dependencies'] = [[enforceList(dep['dep'])[i]['@type'],\n",
    "                                 enforceList(dep['dep'])[i]['governor']['#text'],\n",
    "                                 enforceList(dep['dep'])[i]['dependent']['#text']]\n",
    "                                for dep in raw_sent_list[id]['dependencies'] if dep.get('dep')\n",
    "                                for i in xrange(len(enforceList(dep['dep'])))\n",
    "                                if dep['@type'] == 'basic-dependencies']\n",
    "        sentences.append(sent)\n",
    "\n",
    "    if coref_flag:\n",
    "        results = {'coref': coref_list, 'sentences': sentences}\n",
    "    else:\n",
    "        results = {'sentences': sentences}\n",
    "\n",
    "    if file_name:\n",
    "        results['file_name'] = file_name\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def parse_xml_output(input_dir, corenlp_path=DIRECTORY, memory=\"3g\", raw_output=False, properties='default.properties'):\n",
    "    \"\"\"Because interaction with the command-line interface of the CoreNLP\n",
    "    tools is limited to very short text bits, it is necessary to parse xml\n",
    "    output\"\"\"\n",
    "    #First, we change to the directory where we place the xml files from the\n",
    "    #parser:\n",
    "\n",
    "    xml_dir = tempfile.mkdtemp()\n",
    "    file_list = tempfile.NamedTemporaryFile()\n",
    "\n",
    "    #we get a list of the cleaned files that we want to parse:\n",
    "\n",
    "    files = [input_dir + '/' + f for f in os.listdir(input_dir)]\n",
    "\n",
    "    #creating the file list of files to parse\n",
    "\n",
    "    file_list.write('\\n'.join(files))\n",
    "    file_list.seek(0)\n",
    "\n",
    "    command = init_corenlp_command(corenlp_path, memory, properties)\\\n",
    "        + ' -filelist %s -outputDirectory %s' % (file_list.name, xml_dir)\n",
    "\n",
    "    #creates the xml file of parser output:\n",
    "\n",
    "    call(command, shell=True)\n",
    "\n",
    "    #reading in the raw xml file:\n",
    "    # result = []\n",
    "    try:\n",
    "        for output_file in os.listdir(xml_dir):\n",
    "            with open(xml_dir + '/' + output_file, 'r') as xml:\n",
    "                # parsed = xml.read()\n",
    "                file_name = re.sub('.xml$', '', os.path.basename(output_file))\n",
    "                # result.append(parse_parser_xml_results(xml.read(), file_name,\n",
    "                #                                        raw_output=raw_output))\n",
    "                yield parse_parser_xml_results(xml.read(), file_name,\n",
    "                                               raw_output=raw_output)\n",
    "    finally:\n",
    "        file_list.close()\n",
    "        shutil.rmtree(xml_dir)\n",
    "    # return result\n",
    "\n",
    "\n",
    "class StanfordCoreNLP:\n",
    "\n",
    "    \"\"\"\n",
    "    Command-line interaction with Stanford's CoreNLP java utilities.\n",
    "    Can be run as a JSON-RPC server or imported as a module.\n",
    "    \"\"\"\n",
    "\n",
    "    def _spawn_corenlp(self):\n",
    "        if VERBOSE:\n",
    "            print self.start_corenlp\n",
    "        self.corenlp = pexpect.spawn(self.start_corenlp, timeout=60, maxread=8192, searchwindowsize=80)\n",
    "\n",
    "        # show progress bar while loading the models\n",
    "        if VERBOSE:\n",
    "            widgets = ['Loading Models: ', Fraction()]\n",
    "            pbar = ProgressBar(widgets=widgets, maxval=5, force_update=True).start()\n",
    "            # Model timeouts:\n",
    "            # pos tagger model (~5sec)\n",
    "            # NER-all classifier (~33sec)\n",
    "            # NER-muc classifier (~60sec)\n",
    "            # CoNLL classifier (~50sec)\n",
    "            # PCFG (~3sec)\n",
    "            timeouts = [20, 200, 600, 600, 20]\n",
    "            for i in xrange(5):\n",
    "                self.corenlp.expect(\"done.\", timeout=timeouts[i])  # Load model\n",
    "                pbar.update(i + 1)\n",
    "            self.corenlp.expect(\"Entering interactive shell.\")\n",
    "            pbar.finish()\n",
    "\n",
    "        # interactive shell\n",
    "        self.corenlp.expect(\"\\nNLP> \")\n",
    "\n",
    "    def __init__(self, corenlp_path=DIRECTORY, memory=\"3g\", properties='default.properties', serving=False):\n",
    "        \"\"\"\n",
    "        Checks the location of the jar files.\n",
    "        Spawns the server as a process.\n",
    "        \"\"\"\n",
    "\n",
    "        # spawn the server\n",
    "        self.serving = serving\n",
    "        self.start_corenlp = init_corenlp_command(corenlp_path, memory, properties)\n",
    "        self._spawn_corenlp()\n",
    "\n",
    "    def close(self, force=True):\n",
    "        self.corenlp.terminate(force)\n",
    "\n",
    "    def isalive(self):\n",
    "        return self.corenlp.isalive()\n",
    "\n",
    "    def __del__(self):\n",
    "        # If our child process is still around, kill it\n",
    "        if self.isalive():\n",
    "            self.close()\n",
    "\n",
    "    def _parse(self, text):\n",
    "        \"\"\"\n",
    "        This is the core interaction with the parser.\n",
    "\n",
    "        It returns a Python data-structure, while the parse()\n",
    "        function returns a JSON object\n",
    "        \"\"\"\n",
    "\n",
    "        # CoreNLP interactive shell cannot recognize newline\n",
    "        if '\\n' in text or '\\r' in text:\n",
    "            to_send = re.sub(\"[\\r\\n]\", \" \", text).strip()\n",
    "        else:\n",
    "            to_send = text\n",
    "\n",
    "        # clean up anything leftover\n",
    "        def clean_up():\n",
    "            while True:\n",
    "                try:\n",
    "                    self.corenlp.read_nonblocking(8192, 0.1)\n",
    "                except pexpect.TIMEOUT:\n",
    "                    break\n",
    "        clean_up()\n",
    "\n",
    "        self.corenlp.sendline(to_send)\n",
    "\n",
    "        # How much time should we give the parser to parse it?\n",
    "        # the idea here is that you increase the timeout as a\n",
    "        # function of the text's length.\n",
    "        # max_expected_time = max(5.0, 3 + len(to_send) / 5.0)\n",
    "        max_expected_time = max(300.0, len(to_send) / 3.0)\n",
    "\n",
    "        # repeated_input = self.corenlp.except(\"\\n\")  # confirm it\n",
    "        t = self.corenlp.expect([\"\\nNLP> \", pexpect.TIMEOUT, pexpect.EOF,\n",
    "                                 \"\\nWARNING: Parsing of sentence failed, possibly because of out of memory.\"],\n",
    "                                timeout=max_expected_time)\n",
    "        incoming = self.corenlp.before\n",
    "        if t == 1:\n",
    "            # TIMEOUT, clean up anything left in buffer\n",
    "            clean_up()\n",
    "            print >>sys.stderr, {'error': \"timed out after %f seconds\" % max_expected_time,\n",
    "                                 'input': to_send,\n",
    "                                 'output': incoming}\n",
    "            raise TimeoutError(\"Timed out after %d seconds\" % max_expected_time)\n",
    "        elif t == 2:\n",
    "            # EOF, probably crash CoreNLP process\n",
    "            print >>sys.stderr, {'error': \"CoreNLP terminates abnormally while parsing\",\n",
    "                                 'input': to_send,\n",
    "                                 'output': incoming}\n",
    "            raise ProcessError(\"CoreNLP process terminates abnormally while parsing\")\n",
    "        elif t == 3:\n",
    "            # out of memory\n",
    "            print >>sys.stderr, {'error': \"WARNING: Parsing of sentence failed, possibly because of out of memory.\",\n",
    "                                 'input': to_send,\n",
    "                                 'output': incoming}\n",
    "            raise OutOfMemoryError\n",
    "\n",
    "        if VERBOSE:\n",
    "            print \"%s\\n%s\" % ('=' * 40, incoming)\n",
    "        try:\n",
    "            results = parse_parser_results(incoming)\n",
    "        except Exception as e:\n",
    "            if VERBOSE:\n",
    "                print traceback.format_exc()\n",
    "            raise e\n",
    "\n",
    "        return results\n",
    "\n",
    "    def raw_parse(self, text):\n",
    "        \"\"\"\n",
    "        This function takes a text string, sends it to the Stanford parser,\n",
    "        reads in the result, parses the results and returns a list\n",
    "        with one dictionary entry for each parsed sentence.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            r = self._parse(text)\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            print e  # Should probably log somewhere instead of printing\n",
    "            self.corenlp.close()\n",
    "            self._spawn_corenlp()\n",
    "            if self.serving:  # We don't want to raise the exception when acting as a server\n",
    "                return []\n",
    "            raise e\n",
    "\n",
    "    def parse(self, text):\n",
    "        \"\"\"\n",
    "        This function takes a text string, sends it to the Stanford parser,\n",
    "        reads in the result, parses the results and returns a list\n",
    "        with one dictionary entry for each parsed sentence, in JSON format.\n",
    "        \"\"\"\n",
    "        return json.dumps(self.raw_parse(text))\n",
    "\n",
    "\n",
    "def batch_parse(input_folder, corenlp_path=DIRECTORY, memory=\"3g\", raw_output=False):\n",
    "    \"\"\"\n",
    "    This function takes input files,\n",
    "    sends list of input files to the Stanford parser,\n",
    "    reads in the results from temporary folder in your OS and\n",
    "    returns a generator object of list that consist of dictionary entry.\n",
    "    If raw_output is true, the dictionary returned will correspond exactly to XML.\n",
    "    ( The function needs xmltodict,\n",
    "    and doesn't need init 'StanfordCoreNLP' class. )\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_folder):\n",
    "        raise Exception(\"input_folder does not exist\")\n",
    "\n",
    "    return parse_xml_output(input_folder, corenlp_path, memory, raw_output=raw_output)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    The code below starts an JSONRPC server\n",
    "    \"\"\"\n",
    "    from jsonrpclib.SimpleJSONRPCServer import SimpleJSONRPCServer\n",
    "    parser = optparse.OptionParser(usage=\"%prog [OPTIONS]\")\n",
    "    parser.add_option('-p', '--port', default='8080',\n",
    "                      help='Port to serve on (default 8080)')\n",
    "    parser.add_option('-H', '--host', default='127.0.0.1',\n",
    "                      help='Host to serve on (default localhost; 0.0.0.0 to make public)')\n",
    "    parser.add_option('-q', '--quiet', action='store_false', default=True, dest='verbose',\n",
    "                      help=\"Quiet mode, don't print status msgs to stdout\")\n",
    "    parser.add_option('-S', '--corenlp', default=DIRECTORY,\n",
    "                      help='Stanford CoreNLP tool directory (default %s)' % DIRECTORY)\n",
    "    parser.add_option('-P', '--properties', default='default.properties',\n",
    "                      help='Stanford CoreNLP properties fieles (default: default.properties)')\n",
    "    options, args = parser.parse_args()\n",
    "    VERBOSE = options.verbose\n",
    "    # server = jsonrpc.Server(jsonrpc.JsonRpc20(),\n",
    "    #                         jsonrpc.TransportTcpIp(addr=(options.host, int(options.port))))\n",
    "    try:\n",
    "        server = SimpleJSONRPCServer((options.host, int(options.port)))\n",
    "\n",
    "        nlp = StanfordCoreNLP(options.corenlp, properties=options.properties, serving=True)\n",
    "        server.register_function(nlp.parse)\n",
    "        server.register_function(nlp.raw_parse)\n",
    "\n",
    "        print 'Serving on http://%s:%s' % (options.host, options.port)\n",
    "        # server.serve()\n",
    "        server.serve_forever()\n",
    "    except KeyboardInterrupt:\n",
    "        print >>sys.stderr, \"Bye.\"\n",
    "        exit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name Fraction",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-594bd97b8749>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mprogressbar\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProgressBar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFraction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0munidecode\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munidecode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msubprocess\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name Fraction"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import optparse\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import traceback\n",
    "import pexpect\n",
    "import tempfile\n",
    "import shutil\n",
    "import re\n",
    "from progressbar import ProgressBar, Fraction\n",
    "from unidecode import unidecode\n",
    "from subprocess import call\n",
    "import glob\n",
    "\n",
    "use_winpexpect = True\n",
    "\n",
    "try:\n",
    "    import winpexpect\n",
    "except ImportError:\n",
    "    use_winpexpect = False\n",
    "\n",
    "VERBOSE = False\n",
    "STATE_START, STATE_TEXT, STATE_WORDS, STATE_TREE, STATE_DEPENDENCY, STATE_COREFERENCE = 0, 1, 2, 3, 4, 5\n",
    "WORD_PATTERN = re.compile('\\[([^\\]]+)\\]')\n",
    "CR_PATTERN = re.compile(r\"\\((\\d*),(\\d)*,\\[(\\d*),(\\d*)\\)\\) -> \\((\\d*),(\\d)*,\\[(\\d*),(\\d*)\\)\\), that is: \\\"(.*)\\\" -> \\\"(.*)\\\"\")\n",
    "\n",
    "DIRECTORY = \"stanford-corenlp-full-2013-06-20\"\n",
    "\n",
    "\n",
    "class bc:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "\n",
    "\n",
    "class ProcessError(Exception):\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n",
    "\n",
    "\n",
    "class ParserError(Exception):\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n",
    "\n",
    "\n",
    "class TimeoutError(Exception):\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n",
    "\n",
    "\n",
    "class OutOfMemoryError(Exception):\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n",
    "\n",
    "\n",
    "def init_corenlp_command(corenlp_path, memory, properties):\n",
    "    \"\"\"\n",
    "    Checks the location of the jar files.\n",
    "    Spawns the server as a process.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Can edit jar constants\n",
    "    jar_mask = \"*.jar\"\n",
    "    jars = glob.glob(os.path.join(corenlp_path, jar_mask))\n",
    "\n",
    "    java_path = \"java\"\n",
    "    classname = \"edu.stanford.nlp.pipeline.StanfordCoreNLP\"\n",
    "    # include the properties file, so you can change defaults\n",
    "    # but any changes in output format will break parse_parser_results()\n",
    "    current_dir_pr =  os.path.join(os.path.dirname(os.path.abspath(__file__)), properties)\n",
    "    if os.path.exists(properties):\n",
    "        props = \"-props %s\" % (properties.replace(\" \", \"\\\\ \"))\n",
    "    elif os.path.exists(current_dir_pr):\n",
    "        props = \"-props %s\" % (current_dir_pr.replace(\" \", \"\\\\ \"))\n",
    "    else:\n",
    "        raise Exception(\"Error! Cannot locate: %s\" % properties)\n",
    "\n",
    "    # add memory limit on JVM\n",
    "    if memory:\n",
    "        limit = \"-Xmx%s\" % memory\n",
    "    else:\n",
    "        limit = \"\"\n",
    "\n",
    "    return \"%s %s -cp %s %s %s\" % (java_path, limit, ':'.join(jars), classname, props)\n",
    "\n",
    "def parse_bracketed(s):\n",
    "    '''Parse word features [abc=... def = ...]\n",
    "    Also manages to parse out features that have XML within them\n",
    "    '''\n",
    "    word = None\n",
    "    attrs = {}\n",
    "    temp = {}\n",
    "    # Substitute XML tags, to replace them later\n",
    "    for i, tag in enumerate(re.findall(r\"(<[^<>]+>.*<\\/[^<>]+>)\", s)):\n",
    "        temp[\"^^^%d^^^\" % i] = tag\n",
    "        s = s.replace(tag, \"^^^%d^^^\" % i)\n",
    "    # Load key-value pairs, substituting as necessary\n",
    "    for attr, val in re.findall(r\"([^=\\s]*)=([^\\s]*)\", s):\n",
    "        if val in temp:\n",
    "            val = remove_escapes(temp[val])\n",
    "        if attr == 'Text':\n",
    "            word = remove_escapes(val)\n",
    "        else:\n",
    "            attrs[attr] = remove_escapes(val)\n",
    "    return (word, attrs)\n",
    "\n",
    "\n",
    "def parse_parser_results(text):\n",
    "    \"\"\" This is the nasty bit of code to interact with the command-line\n",
    "    interface of the CoreNLP tools.  Takes a string of the parser results\n",
    "    and then returns a Python list of dictionaries, one for each parsed\n",
    "    sentence.\n",
    "    \"\"\"\n",
    "    results = {\"sentences\": []}\n",
    "    state = STATE_START\n",
    "    lines = unidecode(text.decode('utf-8')).split(\"\\n\")\n",
    "    for index, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "\n",
    "        if line.startswith(\"Sentence #\"):\n",
    "            sentence = {'words': [], 'parsetree': [], 'dependencies': []}\n",
    "            results[\"sentences\"].append(sentence)\n",
    "            state = STATE_TEXT\n",
    "\n",
    "        elif state == STATE_TEXT:\n",
    "            sentence['text'] = remove_escapes(line)\n",
    "            state = STATE_WORDS\n",
    "\n",
    "        elif state == STATE_WORDS:\n",
    "            if not line.startswith(\"[Text=\"):\n",
    "                raise ParserError('Parse error. Could not find \"[Text=\" in: %s' % line)\n",
    "            for s in WORD_PATTERN.findall(line):\n",
    "                sentence['words'].append(parse_bracketed(s))\n",
    "            if not lines[index + 1].startswith(\"[Text=\"):\n",
    "                state = STATE_DEPENDENCY\n",
    "                # skipping TREE because the new depparse annotator doesn't make a parse tree\n",
    "\n",
    "        \n",
    "        elif state == STATE_DEPENDENCY:\n",
    "            if len(line) == 0:\n",
    "                state = STATE_COREFERENCE\n",
    "            else:\n",
    "                split_entry = re.split(\"\\(|, |-\", line[:-1])\n",
    "                if len(split_entry) == 5:\n",
    "                    rel, left, leftindex, right, rightindex = split_entry\n",
    "                    leftindex = re.sub(\"[^0-9]\", \"\", leftindex)\n",
    "                    rightindex = re.sub(\"[^0-9]\", \"\", rightindex)\n",
    "                    sentence['dependencies'].append(tuple([rel,\n",
    "                        remove_escapes(left), leftindex, remove_escapes(right),\n",
    "                        rightindex]))\n",
    "\n",
    "        elif state == STATE_COREFERENCE:\n",
    "            if \"Coreference set\" in line:\n",
    "                if 'coref' not in results:\n",
    "                    results['coref'] = []\n",
    "                coref_set = []\n",
    "                results['coref'].append(coref_set)\n",
    "            else:\n",
    "                for src_i, src_pos, src_l, src_r, sink_i, sink_pos, sink_l, sink_r, src_word, sink_word in CR_PATTERN.findall(line):\n",
    "                    src_i, src_pos, src_l, src_r = int(src_i) - 1, int(src_pos) - 1, int(src_l) - 1, int(src_r) - 1\n",
    "                    sink_i, sink_pos, sink_l, sink_r = int(sink_i) - 1, int(sink_pos) - 1, int(sink_l) - 1, int(sink_r) - 1\n",
    "                    coref_set.append(((src_word, src_i, src_pos, src_l, src_r), (sink_word, sink_i, sink_pos, sink_l, sink_r)))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def parse_parser_xml_results(xml, file_name=\"\", raw_output=False):\n",
    "    import xmltodict\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    def extract_words_from_xml(sent_node):\n",
    "        exted = map(lambda x: x['word'], sent_node['tokens']['token'])\n",
    "        return exted\n",
    "\n",
    "    # Turning the raw xml into a raw python dictionary:\n",
    "    raw_dict = xmltodict.parse(xml)\n",
    "    if raw_output:\n",
    "        return raw_dict\n",
    "\n",
    "    document = raw_dict[u'root'][u'document']\n",
    "\n",
    "    # Making a raw sentence list of dictionaries:\n",
    "    raw_sent_list = document[u'sentences'][u'sentence']\n",
    "\n",
    "    # Convert sentences to the format like python\n",
    "    # TODO: If there is only one sentence in input sentence,\n",
    "    # raw_sent_list is dict and cannot decode following code...\n",
    "    sentences = [{'dependencies': [[dep['dep'][i]['@type'],\n",
    "                                    dep['dep'][i]['governor']['#text'],\n",
    "                                    dep['dep'][i]['governor']['@idx'],\n",
    "                                    dep['dep'][i]['dependent']['#text'],\n",
    "                                    dep['dep'][i]['dependent']['@idx']]\n",
    "                                   for dep in raw_sent_list[j][u'dependencies']\n",
    "                                   if 'dep' in dep\n",
    "                                   for i in xrange(len(dep['dep']))\n",
    "                                   if dep['@type'] == 'collapsed-ccprocessed-dependencies'],\n",
    "                  'text': extract_words_from_xml(raw_sent_list[j]),\n",
    "                  'parsetree': str(raw_sent_list[j]['parse']),\n",
    "                  'words': [[str(token['word']), OrderedDict([\n",
    "                      ('CharacterOffsetEnd', str(token['CharacterOffsetEnd'])),\n",
    "                      ('CharacterOffsetBegin', str(token['CharacterOffsetBegin'])),\n",
    "                      ('PartOfSpeech', str(token['POS'])),\n",
    "                      ('Lemma', str(token['lemma']))])]\n",
    "                  for index, token in enumerate(raw_sent_list[j][u'tokens'][u'token'])]}\n",
    "\n",
    "                 for j in xrange(len(raw_sent_list))]\n",
    "\n",
    "\n",
    "    results = {'sentences': sentences}\n",
    "\n",
    "    if file_name:\n",
    "        results['file_name'] = file_name\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def parse_xml_output(input_dir, corenlp_path=DIRECTORY, memory=\"3g\", raw_output=False, properties='default.properties'):\n",
    "    \"\"\"Because interaction with the command-line interface of the CoreNLP\n",
    "    tools is limited to very short text bits, it is necessary to parse xml\n",
    "    output\"\"\"\n",
    "    #First, we change to the directory where we place the xml files from the\n",
    "    #parser:\n",
    "\n",
    "    xml_dir = tempfile.mkdtemp()\n",
    "    file_list = tempfile.NamedTemporaryFile()\n",
    "\n",
    "    #we get a list of the cleaned files that we want to parse:\n",
    "\n",
    "    files = [os.path.join(input_dir , f) for f in os.listdir(input_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "    #creating the file list of files to parse\n",
    "\n",
    "    file_list.write('\\n'.join(files))\n",
    "    file_list.seek(0)\n",
    "\n",
    "    command = init_corenlp_command(corenlp_path, memory, properties)\\\n",
    "        + ' -filelist %s -outputDirectory %s' % (file_list.name, xml_dir)\n",
    "\n",
    "    #creates the xml file of parser output:\n",
    "\n",
    "    call(command, shell=True)\n",
    "\n",
    "    #reading in the raw xml file:\n",
    "    # result = []\n",
    "    try:\n",
    "        for output_file in os.listdir(xml_dir):\n",
    "            with open(os.path.join(xml_dir + output_file), 'r') as xml:\n",
    "                # parsed = xml.read()\n",
    "                file_name = re.sub('.xml$', '', os.path.basename(output_file))\n",
    "                # result.append(parse_parser_xml_results(xml.read(), file_name,\n",
    "                #                                        raw_output=raw_output))\n",
    "                yield parse_parser_xml_results(xml.read(), file_name,\n",
    "                                               raw_output=raw_output)\n",
    "    finally:\n",
    "        file_list.close()\n",
    "        shutil.rmtree(xml_dir)\n",
    "    # return result\n",
    "\n",
    "\n",
    "class StanfordCoreNLP:\n",
    "\n",
    "    \"\"\"\n",
    "    Command-line interaction with Stanford's CoreNLP java utilities.\n",
    "    Can be run as a JSON-RPC server or imported as a module.\n",
    "    \"\"\"\n",
    "\n",
    "    def _spawn_corenlp(self):\n",
    "        if VERBOSE:\n",
    "            print self.start_corenlp\n",
    "        if use_winpexpect:\n",
    "            self.corenlp = winpexpect.winspawn(self.start_corenlp, maxread=8192,\n",
    "                searchwindowsize=80)\n",
    "        else:\n",
    "            self.corenlp = pexpect.spawn(self.start_corenlp, maxread=8192,\n",
    "                searchwindowsize=80)\n",
    "\n",
    "        # show progress bar while loading the models\n",
    "        if VERBOSE:\n",
    "            widgets = ['Loading Models: ', Fraction()]\n",
    "            pbar = ProgressBar(widgets=widgets, maxval=5, force_update=True).start()\n",
    "            # Model timeouts:\n",
    "            # pos tagger model (~5sec)\n",
    "            # NER-all classifier (~33sec)\n",
    "            # NER-muc classifier (~60sec)\n",
    "            # CoNLL classifier (~50sec)\n",
    "            # PCFG (~3sec)\n",
    "            timeouts = [20, 200, 600, 600, 20]\n",
    "            for i in xrange(5):\n",
    "                self.corenlp.expect(\"done.\", timeout=timeouts[i])  # Load model\n",
    "                pbar.update(i + 1)\n",
    "            self.corenlp.expect(\"Entering interactive shell.\")\n",
    "            pbar.finish()\n",
    "\n",
    "        # interactive shell\n",
    "        self.corenlp.expect(\"\\nNLP> \")\n",
    "\n",
    "    def __init__(self, corenlp_path=DIRECTORY, memory=\"3g\", properties='default.properties', serving=False):\n",
    "        \"\"\"\n",
    "        Checks the location of the jar files.\n",
    "        Spawns the server as a process.\n",
    "        \"\"\"\n",
    "\n",
    "        # spawn the server\n",
    "        self.serving = serving\n",
    "        self.start_corenlp = init_corenlp_command(corenlp_path, memory, properties)\n",
    "        self._spawn_corenlp()\n",
    "\n",
    "    def close(self, force=True):\n",
    "        global use_winpexpect\n",
    "        if use_winpexpect:\n",
    "            self.corenlp.terminate()\n",
    "        else:\n",
    "            self.corenlp.terminate(force)\n",
    "\n",
    "\n",
    "    def isalive(self):\n",
    "        return self.corenlp.isalive()\n",
    "\n",
    "    def __del__(self):\n",
    "        # If our child process is still around, kill it\n",
    "        if self.isalive():\n",
    "            self.close()\n",
    "\n",
    "    def _parse(self, text):\n",
    "        \"\"\"\n",
    "        This is the core interaction with the parser.\n",
    "        It returns a Python data-structure, while the parse()\n",
    "        function returns a JSON object\n",
    "        \"\"\"\n",
    "\n",
    "        # CoreNLP interactive shell cannot recognize newline\n",
    "        if '\\n' in text or '\\r' in text:\n",
    "            to_send = re.sub(\"[\\r\\n]\", \" \", text).strip()\n",
    "        else:\n",
    "            to_send = text\n",
    "\n",
    "        # clean up anything leftover\n",
    "        def clean_up():\n",
    "            while True:\n",
    "                try:\n",
    "                    self.corenlp.read_nonblocking(8192, 0.1)\n",
    "                except pexpect.TIMEOUT:\n",
    "                    break\n",
    "        clean_up()\n",
    "\n",
    "        self.corenlp.sendline(to_send)\n",
    "\n",
    "        # How much time should we give the parser to parse it?\n",
    "        # the idea here is that you increase the timeout as a\n",
    "        # function of the text's length.\n",
    "        # max_expected_time = max(5.0, 3 + len(to_send) / 5.0)\n",
    "        max_expected_time = max(300.0, len(to_send) / 3.0)\n",
    "\n",
    "        # repeated_input = self.corenlp.except(\"\\n\")  # confirm it\n",
    "        t = self.corenlp.expect([\"\\nNLP> \", pexpect.TIMEOUT, pexpect.EOF,\n",
    "                                 \"\\nWARNING: Parsing of sentence failed, possibly because of out of memory.\"],\n",
    "                                timeout=max_expected_time)\n",
    "        incoming = self.corenlp.before\n",
    "        if t == 1:\n",
    "            # TIMEOUT, clean up anything left in buffer\n",
    "            clean_up()\n",
    "            print >>sys.stderr, {'error': \"timed out after %f seconds\" % max_expected_time,\n",
    "                                 'input': to_send,\n",
    "                                 'output': incoming}\n",
    "            raise TimeoutError(\"Timed out after %d seconds\" % max_expected_time)\n",
    "        elif t == 2:\n",
    "            # EOF, probably crash CoreNLP process\n",
    "            print >>sys.stderr, {'error': \"CoreNLP terminates abnormally while parsing\",\n",
    "                                 'input': to_send,\n",
    "                                 'output': incoming}\n",
    "            raise ProcessError(\"CoreNLP process terminates abnormally while parsing\")\n",
    "        elif t == 3:\n",
    "            # out of memory\n",
    "            print >>sys.stderr, {'error': \"WARNING: Parsing of sentence failed, possibly because of out of memory.\",\n",
    "                                 'input': to_send,\n",
    "                                 'output': incoming}\n",
    "            raise OutOfMemoryError\n",
    "\n",
    "        if VERBOSE:\n",
    "            print \"%s\\n%s\" % ('=' * 40, incoming)\n",
    "        try:\n",
    "            results = parse_parser_results(incoming)\n",
    "        except Exception as e:\n",
    "            if VERBOSE:\n",
    "                print traceback.format_exc()\n",
    "            raise e\n",
    "\n",
    "        return results\n",
    "\n",
    "    def raw_parse(self, text):\n",
    "        \"\"\"\n",
    "        This function takes a text string, sends it to the Stanford parser,\n",
    "        reads in the result, parses the results and returns a list\n",
    "        with one dictionary entry for each parsed sentence.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            r = self._parse(text)\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            print e  # Should probably log somewhere instead of printing\n",
    "            self.corenlp.close()\n",
    "            self._spawn_corenlp()\n",
    "            if self.serving:  # We don't want to raise the exception when acting as a server\n",
    "                return []\n",
    "            raise e\n",
    "\n",
    "    def parse(self, text):\n",
    "        \"\"\"\n",
    "        This function takes a text string, sends it to the Stanford parser,\n",
    "        reads in the result, parses the results and returns a list\n",
    "        with one dictionary entry for each parsed sentence, in JSON format.\n",
    "        \"\"\"\n",
    "        return json.dumps(self.raw_parse(text))\n",
    "\n",
    "\n",
    "def batch_parse(input_folder, corenlp_path=DIRECTORY, memory=\"3g\", raw_output=False):\n",
    "    \"\"\"\n",
    "    This function takes input files,\n",
    "    sends list of input files to the Stanford parser,\n",
    "    reads in the results from temporary folder in your OS and\n",
    "    returns a generator object of list that consist of dictionary entry.\n",
    "    If raw_output is true, the dictionary returned will correspond exactly to XML.\n",
    "    ( The function needs xmltodict,\n",
    "    and doesn't need init 'StanfordCoreNLP' class. )\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_folder):\n",
    "        raise Exception(\"input_folder does not exist\")\n",
    "\n",
    "    return parse_xml_output(input_folder, corenlp_path, memory, raw_output=raw_output)\n",
    "\n",
    "def remove_escapes(text):\n",
    "    \"\"\"Given a string, remove PTB3 escape characters.\n",
    "    \"\"\"\n",
    "    escapes = {\"-lrb-\": \"(\",\n",
    "        \"-rrb-\": \")\",\n",
    "        \"-lsb-\": \"[\",\n",
    "        \"-rsb-\": \"]\",\n",
    "        \"-lcb-\": \"{\",\n",
    "        \"-rcb-\": \"}\",\n",
    "        \"-LRB-\": \"(\",\n",
    "        \"-RRB-\": \")\",\n",
    "        \"-LSB-\": \"[\",\n",
    "        \"-RSB-\": \"]\",\n",
    "        \"-LCB-\": \"{\",\n",
    "        \"-RCB-\": \"}\"}\n",
    "    if text:\n",
    "        pattern = re.compile('|'.join(re.escape(key) for key in escapes.keys()))\n",
    "        return pattern.sub(lambda x: escapes[x.group()], text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    The code below starts an JSONRPC server\n",
    "    \"\"\"\n",
    "    from jsonrpclib.SimpleJSONRPCServer import SimpleJSONRPCServer\n",
    "    parser = optparse.OptionParser(usage=\"%prog [OPTIONS]\")\n",
    "    parser.add_option('-p', '--port', default='8080',\n",
    "                      help='Port to serve on (default 8080)')\n",
    "    parser.add_option('-H', '--host', default='127.0.0.1',\n",
    "                      help='Host to serve on (default localhost; 0.0.0.0 to make public)')\n",
    "    parser.add_option('-q', '--quiet', action='store_false', default=True, dest='verbose',\n",
    "                      help=\"Quiet mode, don't print status msgs to stdout\")\n",
    "    parser.add_option('-S', '--corenlp', default=DIRECTORY,\n",
    "                      help='Stanford CoreNLP tool directory (default %s)' % DIRECTORY)\n",
    "    parser.add_option('-P', '--properties', default='default.properties',\n",
    "                      help='Stanford CoreNLP properties fieles (default: default.properties)')\n",
    "    options, args = parser.parse_args()\n",
    "    VERBOSE = options.verbose\n",
    "    # server = jsonrpc.Server(jsonrpc.JsonRpc20(),\n",
    "    #                         jsonrpc.TransportTcpIp(addr=(options.host, int(options.port))))\n",
    "    try:\n",
    "        server = SimpleJSONRPCServer((options.host, int(options.port)))\n",
    "\n",
    "        nlp = StanfordCoreNLP(options.corenlp, properties=options.properties, serving=True)\n",
    "        server.register_function(nlp.parse)\n",
    "        server.register_function(nlp.raw_parse)\n",
    "\n",
    "        print 'Serving on http://%s:%s' % (options.host, options.port)\n",
    "        # server.serve()\n",
    "        server.serve_forever()\n",
    "    except KeyboardInterrupt:\n",
    "        print >>sys.stderr, \"Bye.\"\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
