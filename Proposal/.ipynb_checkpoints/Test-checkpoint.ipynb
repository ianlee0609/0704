{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#安裝套件\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "from stemming.porter2 import stem\n",
    "\n",
    "\n",
    "knzr = TweetTokenizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stop = get_stop_words('en') \n",
    "lemma = nltk.wordnet.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'Peyton', u'Reed']]\n"
     ]
    }
   ],
   "source": [
    "#電影data\n",
    "from collections import Counter\n",
    "\n",
    "import pyodbc\n",
    "conn = pyodbc.connect('DRIVER={SQL Server};SERVER=163.14.68.48;DATABASE=FB_IMDB;UID=sa;PWD=scucc')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"select * from Moviedata where   [movieNameE]='Ant-Man' \")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "\n",
    "\n",
    "Stars= rows[0][-1].split(',')\n",
    "Writer=rows[0][-2].split(',')\n",
    "Director=rows[0][-3].split(',')\n",
    "\n",
    "MovieName=rows[0][1]\n",
    "StarsDictionary=[]\n",
    "WritersDictionary=[]\n",
    "DirectorsDictionary=[]\n",
    "for Star in Stars:\n",
    "    Ssplit= Star.strip().split(' ')\n",
    "    StarsDictionary.append(Ssplit)\n",
    "for Star in Writer:\n",
    "    Ssplit= Star.strip().split(' ')\n",
    "    WritersDictionary.append(Ssplit)\n",
    "for Star in Director:\n",
    "    Ssplit= Star.strip().split(' ')\n",
    "    DirectorsDictionary.append(Ssplit)\n",
    "    \n",
    "print DirectorsDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2517\n"
     ]
    }
   ],
   "source": [
    "#FBPost搜尋\n",
    "import pyodbc\n",
    "conn = pyodbc.connect('DRIVER={SQL Server};SERVER=163.14.68.48;DATABASE=FB_IMDB;UID=sa;PWD=scucc')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"select * from FBPost  where  [message] like '%Ant-Man%trailer%'  \")\n",
    "rows = cursor.fetchall()\n",
    "PostNO=rows[-2][0]\n",
    "Like=rows[-2][3]\n",
    "Coment=rows[-2][4]\n",
    "print PostNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'OA', u'accept', u'accept')\n"
     ]
    }
   ],
   "source": [
    "#Aspect搜尋\n",
    "import pyodbc\n",
    "conn = pyodbc.connect('DRIVER={SQL Server};SERVER=163.14.68.48;DATABASE=FB_IMDB;UID=sa;PWD=scucc')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"select * from [Aspect] \")\n",
    "AspectRows = cursor.fetchall()\n",
    "print AspectRows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'simple', 1.0)\n"
     ]
    }
   ],
   "source": [
    "#Opinion搜尋\n",
    "import pyodbc\n",
    "conn = pyodbc.connect('DRIVER={SQL Server};SERVER=163.14.68.48;DATABASE=FB_IMDB;UID=sa;PWD=scucc')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"select * from [Opinion] \")\n",
    "OpinionRows = cursor.fetchall()\n",
    "print OpinionRows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Emotion SQLSEVER\n",
    "import pyodbc\n",
    "conn = pyodbc.connect('DRIVER={SQL Server};SERVER=163.14.68.48;DATABASE=FB_IMDB;UID=sa;PWD=scucc')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"select * from [Emotion] \")\n",
    "EmotionRows = cursor.fetchall()\n",
    "\n",
    "\n",
    "def Emotion_Detect(Word):\n",
    "    P=0\n",
    "    N=0\n",
    "    for EmotionRow in EmotionRows:\n",
    "        \n",
    "        if EmotionRow[2] in Word:\n",
    "            if EmotionRow[4]==1 :\n",
    "                P+=1\n",
    "                #print Word\n",
    "                break\n",
    "            if EmotionRow[4]==-1:\n",
    "                N+=1\n",
    "                #print Word\n",
    "    EmotionArray=[P,N]\n",
    "    return  np.array(EmotionArray)\n",
    "    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "#Slang SQLSEVER\n",
    "import pyodbc\n",
    "conn = pyodbc.connect('DRIVER={SQL Server};SERVER=163.14.68.48;DATABASE=FB_IMDB;UID=sa;PWD=scucc')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"select * from Slang \")\n",
    "SlangRows = cursor.fetchall()\n",
    "\n",
    "def Slang_Detect(Word):\n",
    "    \n",
    "    P=0\n",
    "    N=0\n",
    "    for SlangRow in SlangRows:\n",
    "        if SlangRow[0].lower() == Word:\n",
    "            \n",
    "            if SlangRow[3]==1 :\n",
    "                P+=1\n",
    "                SlangPWord[Word]+=1\n",
    "                \n",
    "                break\n",
    "            if SlangRow[3]==-1:\n",
    "                N+=1\n",
    "                SlangNWord[Word]+=1\n",
    "                break\n",
    "            \n",
    "    SlangArray=[P,N]\n",
    "    return  np.array(SlangArray)\n",
    "print Slang_Detect('lol')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessEmotion(Comment):\n",
    "#     print \"原始\"\n",
    "#     print Comment\n",
    "    Comment= Comment.lower()\n",
    "#     print \"轉小寫\"\n",
    "#     print Comment\n",
    "    sent=re.split(r'[,.!?]+', Comment)\n",
    "#     print \"斷句\"\n",
    "#     print sent\n",
    "    for i in range (0,len(sent)):\n",
    "        toeken1= sent[i].split(\" \")\n",
    "        sent[i]=toeken1\n",
    "#     print \"依空白切字\"\n",
    "#     print sent   \n",
    "    stop(sent)\n",
    "#     print \"去除STOPWORD\"\n",
    "#     print sent\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessSlang(Comment):\n",
    "\n",
    "    Comment= Comment.lower()\n",
    "    sent=re.split(r'[,.!?]+', Comment)\n",
    "    for i in range (0,len(sent)):\n",
    "        toeken1= sent[i].split(\" \")\n",
    "        sent[i]=toeken1\n",
    "  \n",
    "    stop(sent)\n",
    "    repunctForSlang(sent)\n",
    "\n",
    "    for n in sent:\n",
    "        while \"\" in n:\n",
    "            n.remove(\"\")\n",
    "\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessALL(Comment):\n",
    "    Comment= Comment.lower()\n",
    "\n",
    "    sent=re.split(r'[,.!?]+', Comment)\n",
    "\n",
    "    for i in range (0,len(sent)):\n",
    "        toeken1= sent[i].split(\" \")\n",
    "        sent[i]=toeken1\n",
    "    stop(sent)\n",
    "    repunct(sent)\n",
    "#     print \"去掉符號\"\n",
    "#     print sent\n",
    "    \n",
    "    \n",
    "     #去掉沒東西的元素\n",
    "    for n in sent:\n",
    "        while \"\" in n:\n",
    "            n.remove(\"\")\n",
    "#     print \"去掉空白\"\n",
    "#     print sent\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love good \n"
     ]
    }
   ],
   "source": [
    "def adjOnly(Comment):\n",
    "    \n",
    "    sentence=unicode(Comment)\n",
    "    tokens = nlp(sentence)\n",
    "    newComment=\"\"\n",
    "    for tok in tokens:\n",
    "        \n",
    "        if tok.pos_=='ADJ'or tok.pos_=='VERB'or tok.pos_=='INTJ':\n",
    "            newComment+=str(tok)+\" \"\n",
    "    return newComment \n",
    "print adjOnly('I love good you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I Annie movie \n"
     ]
    }
   ],
   "source": [
    "def NounOnly(Comment):\n",
    "    \n",
    "    sentence=unicode(Comment)\n",
    "    tokens = nlp(sentence)\n",
    "    newComment=\"\"\n",
    "    for tok in tokens:\n",
    "        \n",
    "        if tok.pos_=='PROPN'or tok.pos_=='PRON'or tok.pos_=='NOUN'or tok.pos_=='verb':\n",
    "            newComment+=str(tok)+\" \"\n",
    "    return newComment \n",
    "print NounOnly('I love good Annie movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54 30  2  0  9]\n",
      "OA:\n",
      "[('movi', 30), ('film', 4), ('pick', 3), ('hit', 2), ('thank', 2), ('action', 2), ('choic', 1), ('comedi', 1), ('mind', 1), ('piec', 1), ('thought', 1), ('fail', 1), ('stuff', 1), ('hope', 1), ('comic', 1), ('danc', 1), ('problem', 1)]\n",
      "\n",
      "\n",
      "SE:\n",
      "[('cgi', 2)]\n",
      "\n",
      "\n",
      "ST:\n",
      "[('superhero', 6), ('plot', 2), ('joke', 2), ('idea', 2), ('end', 2), ('teen', 1), ('juli', 1), ('writer', 1), ('moment', 1), ('villain', 1), ('charact', 1), ('margaret', 1), ('stori', 1), ('chris', 1), ('brick', 1), ('burgundi', 1)]\n",
      "\n",
      "\n",
      "Opinion:\n",
      "Counter({'pos': 121, 'neg': 49})\n",
      "\n",
      "\n",
      "Emotion:\n",
      "[9 0]\n",
      "\n",
      "\n",
      "Writer:\n",
      "[(u'Edgar', 4), (u'Joe', 1)]\n",
      "\n",
      "\n",
      "Star:\n",
      "[(u'Paul', 6), (u'Rudd', 2), (u'Michael', 1)]\n",
      "\n",
      "\n",
      "撞出意見結果\n",
      "正\n",
      "[('love', 20), ('good', 18), ('great', 12), ('like', 12), ('awesom', 12), ('impress', 10), ('entertain', 10), ('will', 9), ('comic', 6), ('excit', 6), ('better', 6), ('super', 5), ('funni', 5), ('know', 5), ('thought', 4), ('fantast', 4), ('amaz', 4), ('hilari', 4), ('hope', 4), ('deserv', 4), ('make', 3), ('complet', 3), ('come', 3), ('marvel', 3), ('move', 2), ('pass', 2), ('imagin', 2), ('humor', 2), ('fit', 2), ('happi', 2), ('damn', 2), ('regard', 2), ('serious', 2), ('cool', 2), ('brilliant', 2), ('incred', 2), ('learn', 1), ('creat', 1), ('acknowledg', 1), ('close', 1), ('pretti', 1), ('best', 1), ('much', 1), ('rememb', 1), ('suit', 1), ('call', 1), ('real', 1), ('worthwhil', 1), ('favourit', 1), ('new', 1), ('ha', 1), ('lucki', 1), ('funnier', 1), ('epic', 1), ('possibl', 1), ('well', 1), ('portabl', 1), ('optimist', 1), ('fun', 1), ('meet', 1), ('believ', 1)]\n",
      "負\n",
      "[('wait', 8), ('need', 6), ('bad', 6), ('ridicul', 4), ('pass', 4), ('get', 4), ('wrong', 4), ('disturb', 4), ('stupid', 4), ('sad', 3), ('hate', 3), ('weak', 3), ('terribl', 2), ('doubt', 2), ('damn', 2), ('forc', 2), ('fall', 2), ('cool', 2), ('old', 1), ('owe', 1), ('kill', 1), ('close', 1), ('lame', 1), ('hit', 1), ('evil', 1), ('let', 1), ('invis', 1), ('silli', 1), ('die', 1), ('wors', 1), ('plagu', 1), ('weird', 1)]\n",
      "\n",
      "\n",
      "Slang(positive):\n",
      "[(u'lol', 7), (u'hope', 3), (u'gonna', 1), (u'ftw', 1), (u'thx', 1)]\n",
      "Slang(negtive):\n",
      "[(u'omg', 2), (u'du', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python27\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:5: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "#CommentOpinion搜尋\n",
    "import pyodbc\n",
    "conn = pyodbc.connect('DRIVER={SQL Server};SERVER=163.14.68.48;DATABASE=FB_IMDB;UID=sa;PWD=scucc')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"select * from CommentOpinion  where  [PostNO]=? \",PostNO)\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "SlangPWord=Counter()\n",
    "OAWord=Counter()\n",
    "SEWord=Counter()\n",
    "STWord=Counter()\n",
    "StarWord=Counter()\n",
    "WriterWord=Counter()\n",
    "SlangNWord=Counter()\n",
    "OpinionPWord=Counter()\n",
    "OpinionNWord=Counter()\n",
    "\n",
    "\n",
    "Opinion=Counter()\n",
    "Emotion=Counter()\n",
    "\n",
    "starCount=0\n",
    "ALLCountArray=[0,0,0,0,0]\n",
    "EmotionArray=[0,0]\n",
    "SlangArray=[0,0]\n",
    "ALLCountArray=np.array(ALLCountArray)\n",
    "EmotionArray=np.array(EmotionArray)\n",
    "SlangArray=np.array(SlangArray)\n",
    "for row in rows:\n",
    "    P_N=0\n",
    "    \n",
    "    Comment= row[3].lower()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Comment='it seems like every Marvel movie has to have f***ing evil robots to complete the plot. thanks but no thanks, waiting for next superman movie.'\n",
    "    \n",
    "    sent=preprocessEmotion(Comment)\n",
    "    \n",
    "    for sentNO in range (0,len(sent)):\n",
    "        for word in range (0,len(sent[sentNO])):\n",
    "            Emotion= Emotion_Detect(sent[sentNO][word])\n",
    "            EmotionArray+=Emotion\n",
    "            \n",
    "    sent= preprocessSlang(Comment)      \n",
    "            \n",
    "    for sentNO in range (0,len(sent)):\n",
    "        for word in range (0,len(sent[sentNO])):\n",
    "            Word=stem(sent[sentNO][word])\n",
    "            Slang= Slang_Detect(Word)\n",
    "            SlangArray+=Slang\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    Comment= row[3]\n",
    "    Comment=adjOnly(Comment)\n",
    "    sent=preprocessALL(Comment)\n",
    "    \n",
    "    for sentNO in range (0,len(sent)):\n",
    "        for word in range (0,len(sent[sentNO])):\n",
    "           \n",
    "            Word=stem(sent[sentNO][word])\n",
    "            P_N= opinionDetect2(Word)\n",
    "            if P_N==1:\n",
    "                Opinion['pos']+=1\n",
    "            if P_N==-1:\n",
    "                Opinion['neg']+=1\n",
    "                \n",
    "                \n",
    "    Comment= row[3].lower()\n",
    "    Comment=NounOnly(Comment)\n",
    "    \n",
    "    sent=preprocessALL(Comment)\n",
    "                \n",
    "    for sentNO in range (0,len(sent)):\n",
    "        for word in range (0,len(sent[sentNO])):\n",
    "\n",
    "            Word=stem(sent[sentNO][word])       \n",
    "            \n",
    "            AspectArray= AspectDetect(Word)\n",
    "            ALLCountArray+=AspectArray\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    Comment= row[3].lower()\n",
    "    \n",
    "    \n",
    "    NameArray=Namedetect(Comment)\n",
    "    ALLCountArray+=NameArray\n",
    "    \n",
    "    \n",
    "print ALLCountArray    \n",
    "   \n",
    "print \"OA:\"\n",
    "print OAWord.most_common()\n",
    "print \"\\n\"\n",
    "\n",
    "print \"SE:\"\n",
    "print SEWord.most_common()\n",
    "print \"\\n\"\n",
    "\n",
    "print \"ST:\"\n",
    "print STWord.most_common()\n",
    "print \"\\n\"\n",
    "\n",
    "print \"Opinion:\"\n",
    "print Opinion\n",
    "print \"\\n\"\n",
    "\n",
    "print \"Emotion:\"\n",
    "print EmotionArray\n",
    "print \"\\n\"\n",
    "\n",
    "print \"Writer:\"\n",
    "print WriterWord.most_common()\n",
    "print \"\\n\"\n",
    "\n",
    "print \"Star:\"\n",
    "print StarWord.most_common()\n",
    "print \"\\n\"\n",
    "\n",
    "print \"撞出意見結果\"\n",
    "print \"正\"\n",
    "print OpinionPWord.most_common()\n",
    "print \"負\"\n",
    "print OpinionNWord.most_common()\n",
    "print \"\\n\"\n",
    "print \"Slang(positive):\"\n",
    "print SlangPWord.most_common()\n",
    "print \"Slang(negtive):\"\n",
    "print SlangNWord.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#偵測人名\n",
    "\n",
    "def Namedetect(Message):\n",
    "    Message.lower()\n",
    "    starcount=0\n",
    "    Writercount=0\n",
    "    Directorcount=0\n",
    "    for Stars in StarsDictionary:\n",
    "        for  Star in Stars:\n",
    "            if Star.lower() in Message.lower():\n",
    "                \n",
    "                StarWord[Star]+=1\n",
    "                starcount+=1\n",
    "                break\n",
    "    for Stars in WritersDictionary:\n",
    "        for  Star in Stars:\n",
    "            if Star.lower() in Message.lower():\n",
    "                WriterWord[Star]+=1\n",
    "\n",
    "                Writercount+=1\n",
    "                break\n",
    "    for Stars in DirectorsDictionary:\n",
    "        for  Star in Stars:\n",
    "            if Star.lower() in Message.lower():\n",
    "                Directorcount+=1\n",
    "                break\n",
    "    NameArray=[0,Writercount,0,Directorcount,starcount]\n",
    "    return np.array(NameArray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print Namedetect('Peyton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#偵測意見\n",
    "\n",
    "P=[]\n",
    "N=[]\n",
    "for OpinionRow in OpinionRows:\n",
    "    if OpinionRow[1]==1:\n",
    "        N.append(stem(OpinionRow[0].lower()))\n",
    "    if OpinionRow[1]==0:\n",
    "        P.append(stem(OpinionRow[0].lower()))\n",
    "        \n",
    "if ('loves' in P) :\n",
    "    print 1\n",
    "\n",
    "def opinionDetect2(word):\n",
    "    P_N=0\n",
    "    for pos in P:\n",
    "        if word == pos:\n",
    "\n",
    "            P_N=1\n",
    "            OpinionPWord[word]+=1\n",
    "    for neg in N:\n",
    "        if word == neg:\n",
    "            P_N=-1\n",
    "            OpinionNWord[word]+=1\n",
    "    return P_N    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#意見 正負表\n",
    "f = open('D:/DOWNLOAD/opinion-lexicon-English/negative-words.txt', 'r')\n",
    "N = f.read()\n",
    "f.close()\n",
    "f = open('D:/DOWNLOAD/opinion-lexicon-English/positive-words.txt', 'r')\n",
    "P = f.read()\n",
    "f.close()\n",
    "\n",
    "PArray=P.split(\"\\n\")   \n",
    "Array=N.split(\"\\n\")   \n",
    "   \n",
    "NegArray=Array[35:]\n",
    "PosArray=PArray[35:]\n",
    "for x in range(0,len(PosArray)):\n",
    "    PosArray[x]= stem(PosArray[x].lower())\n",
    "for x in range(0,len(NegArray)):\n",
    "    NegArray[x]= stem(NegArray[x].lower())\n",
    "\n",
    "def opinionDetect3(word):\n",
    "    P_N=0\n",
    "    for pos in PosArray:\n",
    "        if word == pos:\n",
    "\n",
    "            P_N=1\n",
    "            OpinionPWord[word]+=1\n",
    "    for neg in NegArray:\n",
    "        if word == neg:\n",
    "            P_N=-1\n",
    "            OpinionNWord[word]+=1\n",
    "    return P_N      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#面向偵測\n",
    "def AspectDetect(word):\n",
    "    OAcount=0\n",
    "    STcount=0\n",
    "    SEcount=0\n",
    "    for  AspectRow in AspectRows:\n",
    "        if  AspectRow[2] == word:\n",
    "            if AspectRow[0]=='PAC' or AspectRow[0]=='PDR':\n",
    "                continue\n",
    "            if  AspectRow[0]=='OA':\n",
    "                OAWord[word]+=1\n",
    "                OAcount+=1\n",
    "            if  AspectRow[0]=='ST':\n",
    "                STWord[word]+=1\n",
    "                STcount+=1\n",
    "            if AspectRow[0]=='SE':\n",
    "                SEWord[word]+=1\n",
    "                SEcount+=1\n",
    "    AspectArray=[OAcount,STcount,SEcount,0,0]\n",
    "    return  np.array(AspectArray)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#去除STOP\n",
    "def stop(sent):\n",
    "    for sentNO in range (0,len(sent)):\n",
    "        for word in range (0,len(sent[sentNO])):\n",
    "            if sent[sentNO][word] in en_stop:\n",
    "                sent[sentNO][word]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#去除符號\n",
    "def repunct(sent):\n",
    "    for sentNO in range (0,len(sent)):\n",
    "        for word in range (0,len(sent[sentNO])):\n",
    "            sent[sentNO][word]=re.sub(r'[^\\w]', '', sent[sentNO][word])\n",
    "            sent[sentNO][word]=re.sub('[0-9]', '', sent[sentNO][word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#去除符號\n",
    "def repunctForSlang(sent):\n",
    "    for sentNO in range (0,len(sent)):\n",
    "        for word in range (0,len(sent[sentNO])):\n",
    "            sent[sentNO][word]=re.sub(r'[^\\w]', '', sent[sentNO][word])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love good \n"
     ]
    }
   ],
   "source": [
    "sentence='i love this good movie'\n",
    "sentence=unicode(sentence, \"utf-8\")\n",
    "tokens = nlp(sentence)\n",
    "newComment=\"\"\n",
    "for tok in tokens:\n",
    "    \n",
    "    if tok.pos_=='ADJ'or tok.pos_=='VERB':\n",
    "        newComment+=str(tok)+\" \"\n",
    "print newComment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem('best'.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
